import math
import re
import nltk
from nltk.tokenize import word_tokenize
import string

# Download required NLTK data (if not already present)
nltk.download('punkt', quiet=True)

def levenshtein_distance(s1: str, s2: str) -> int:
    """
    Compute the Levenshtein edit distance between s1 and s2.
    """
    m, n = len(s1), len(s2)
    dp = [[0]*(n+1) for _ in range(m+1)]
    for i in range(m+1):
        dp[i][0] = i
    for j in range(n+1):
        dp[0][j] = j
    for i in range(1, m+1):
        for j in range(1, n+1):
            cost = 0 if s1[i-1] == s2[j-1] else 1
            dp[i][j] = min(
                dp[i-1][j] + 1,        # deletion
                dp[i][j-1] + 1,        # insertion
                dp[i-1][j-1] + cost    # substitution
            )
    print(f"[Levenshtein] Distance between '{s1}' and '{s2}': {dp[m][n]}")
    return dp[m][n]

def get_kgrams(word: str, k: int = 2) -> set:
    """
    Returns the set of k-grams for the word.
    """
    kgrams = set()
    if len(word) < k:
        kgrams.add(word)
    else:
        for i in range(len(word) - k + 1):
            kgrams.add(word[i:i+k])
    return kgrams
def common_prefix_length(s1: str, s2: str) -> int:
    """
    Returns the number of characters that match from the start of s1 and s2.
    """
    l = 0
    for c1, c2 in zip(s1, s2):
        if c1 == c2:
            l += 1
        else:
            break
    return l

def jaccard_coefficient(word1: str, word2: str, k: int = 2) -> float:
    """
    Computes the Jaccard coefficient between the k-gram sets of two words.
    """
    grams1 = get_kgrams(word1, k)
    grams2 = get_kgrams(word2, k)
    inter = len(grams1.intersection(grams2))
    union = len(grams1.union(grams2))
    return inter / union if union != 0 else 0.0

#############################################
# SimpleSpellingCorrector Class
#############################################

class SimpleSpellingCorrector:
    def __init__(self, vocab: set, k: int = 2, jaccard_threshold: float = 0.3):
        self.vocab = set(vocab)
        self.k = k
        self.jaccard_threshold = jaccard_threshold
        # Precompute index for replacement wildcards.
        self._build_wildcard_indices()

    def _build_wildcard_indices(self):
        """
        Precompute an index for replacement wildcards:
          Keys are patterns of the same length as the word with one letter replaced by '$'.
        """
        self.replacement_index = {}
        for word in self.vocab:
            for i in range(len(word)):
                key = word[:i] + '$' + word[i+1:]
                self.replacement_index.setdefault(key, set()).add(word)
        # Uncomment for debugging:
        # print("Replacement Index:", self.replacement_index)

    def _insertion_candidates_regex(self, word: str) -> set:
        """
        Use a regex to find vocabulary words that are one letter longer than 'word'
        and that become 'word' when one letter is removed.
        """
        pattern_parts = []
        for i in range(len(word) + 1):
            # Build a pattern where an extra letter (any letter) is inserted at position i.
            part = re.escape(word[:i]) + "[A-Za-z]" + re.escape(word[i:])
            pattern_parts.append(part)
        pattern = "^(?:" + "|".join(pattern_parts) + ")$"
        regex = re.compile(pattern)
        print(f"Regex for insertion candidates for '{word}': {pattern}")
        # Only consider vocabulary words exactly one character longer than 'word'
        return {w for w in self.vocab if len(w) == len(word) + 1 and regex.match(w)}

    def _lookup_candidates(self, word: str) -> set:
        """
        Generate wildcard patterns for the input word and use the precomputed index
        and regex-based insertion lookup to quickly gather candidate words.
        
        - Replacement patterns: generated by replacing one letter with '$'
          and using the replacement_index.
        - Deletion patterns: generated by deleting one letter and directly checking the vocab.
        - Insertion candidates: found via a regex that matches vocabulary words
          one letter longer than the input.
        """
        candidates = set()
        
        # Get insertion candidates via regex.
        insertion_candidates = self._insertion_candidates_regex(word)
        
        # Replacement patterns: patterns with length = len(word).
        replacement_patterns = []
        for i in range(len(word)):
            pattern = word[:i] + '$' + word[i+1:]
            replacement_patterns.append(pattern)
            if pattern in self.replacement_index:
                candidates.update(self.replacement_index[pattern])
        
        # Deletion patterns: generate patterns by deleting one letter and directly search in vocab.
        deletion_patterns = []
        if len(word) > 3:
            for i in range(len(word)):
                pattern = word[:i] + word[i+1:]
                deletion_patterns.append(pattern)
                if pattern in self.vocab:
                    candidates.add(pattern)
        
        # Update candidates with insertion candidates found via regex.
        candidates.update(insertion_candidates)
        
        # Print the generated patterns and insertion candidates.
        print(f"Wildcard patterns for '{word}':")
        print(f"  Replacement patterns: {replacement_patterns}")
        print(f"  Deletion patterns: {deletion_patterns}")
        print(f"  Insertion candidates (regex): {list(insertion_candidates)}")
        
        print(f"Candidates for '{word}' from wildcard lookup: {candidates}")
        return candidates

    def correct_word(self, word: str) -> str:
        """
        Correct a single word.
        - If the word is in the vocabulary, return it as is.
        - Otherwise, attempt to retrieve candidates using wildcard lookup.
          If found, choose the candidate with the lowest Levenshtein distance,
          breaking ties by prioritizing candidates that share the first letter.
          If still tied, choose the candidate that matches the most from the start.
        - If no candidates are found via wildcard lookup, fall back to
          words with the same first letter using k-gram (Jaccard) filtering.
        """
        if word in self.vocab:
            print(f"'{word}' is in vocabulary; no correction needed.")
            return word

        candidate_set = self._lookup_candidates(word)
        print(f"Candidates from wildcard lookup for '{word}': {candidate_set}")

        if candidate_set:
            best_candidate = None
            best_distance = None
            for cand in candidate_set:
                dist = levenshtein_distance(word, cand)
                if best_candidate is None:
                    best_candidate = cand
                    best_distance = dist
                elif dist < best_distance:
                    best_candidate = cand
                    best_distance = dist
                elif dist == best_distance:
                    # If only one candidate matches the first letter, choose that one.
                    if cand[0] == word[0] and best_candidate[0] != word[0]:
                        best_candidate = cand
                        best_distance = dist
                    # If both share the first letter, choose the one with the longest common prefix.
                    elif cand[0] == word[0] and best_candidate[0] == word[0]:
                        if common_prefix_length(word, cand) > common_prefix_length(word, best_candidate):
                            best_candidate = cand
                            best_distance = dist
            print(f"Selected candidate from wildcard lookup: {best_candidate} (distance: {best_distance})")
            return best_candidate
        else:
            # Fallback: candidate set from words with the same first letter.
            candidate_set = {w for w in self.vocab if w[0] == word[0]}
            filtered = []
            for cand in candidate_set:
                jc = jaccard_coefficient(word, cand, self.k)
                if jc >= self.jaccard_threshold:
                    filtered.append((cand, jc))
            if filtered:
                filtered.sort(key=lambda x: (-x[1], levenshtein_distance(word, x[0])))
                best_candidate = filtered[0][0]
                print(f"Candidates from fallback (same first letter & k-gram): {filtered}")
                print(f"Selected candidate from fallback: {best_candidate}")
                return best_candidate
            else:
                print(f"No candidates found for '{word}' in fallback; returning original.")
                return word

    def correct_sentence(self, sentence: str) -> str:
        """
        Correct each token in the sentence.
        Uses whitespace tokenization and preserves trailing punctuation.
        """
        tokens = sentence.split()
        corrected_tokens = []
        for token in tokens:
            punct = ""
            while token and not token[-1].isalnum():
                punct = token[-1] + punct
                token = token[:-1]
            corrected = self.correct_word(token.lower())
            if token and token[0].isupper():
                corrected = corrected.capitalize()
            corrected_tokens.append(corrected + punct)
        corrected_sentence = " ".join(corrected_tokens)
        print(f"[Corrected Sentence]: {corrected_sentence}")
        return corrected_sentence

#############################################
#  Main Section
#############################################

if __name__ == "__main__":
    try:
    
        with open("./data/train1.txt", "r", encoding="utf-8") as f:
            training_text1 = f.read()
        with open("./data/train2.txt", "r", encoding="utf-8") as f:
            training_text2 = f.read()
        training_text = training_text1 + training_text2
    except Exception as e:
        print("Error reading training file './data/train1.txt':", e)
        training_text = ""
    
    # tokenizer = ToktokTokenizer()
    
    def nltk_clean_tokenize(text):
        tokens = word_tokenize(text)
        # Reassemble common contractions.
        contraction_suffixes = {"n't", "'s", "'m", "'re", "'ve", "'ll"}
        new_tokens = []
        i = 0
        while i < len(tokens):
            if i < len(tokens) - 1 and tokens[i+1] in contraction_suffixes:
                new_tokens.append(tokens[i] + tokens[i+1])
                i += 2
            else:
                new_tokens.append(tokens[i])
                i += 1
        filtered_tokens = [token for token in new_tokens if re.search(r'[A-Za-z0-9]', token)]
        return filtered_tokens

    training_tokens = nltk_clean_tokenize(training_text.lower())
    training_vocab = set(training_tokens)
    print(f"Training vocabulary size: {len(training_vocab)}")
    
    corrector = SimpleSpellingCorrector(training_vocab, k=2, jaccard_threshold=0.3)
    
    try:
        with open("./data/misspelling_public.txt", "r", encoding="utf-8") as f:
            test_lines = f.readlines()
    except Exception as e:
        print("Error reading test file './data/misspelling_public.txt':", e)
        test_lines = []

    with open("output.txt", "w") as output_file:
        for line in test_lines:
            if not line.strip():
                continue
            if "&&" not in line:
                print(f"Line in unexpected format: {line}")
                continue
            parts = line.split("&&")
            if len(parts) != 2:
                print(f"Line in unexpected format: {line}")
                continue
            correct_text, incorrect_text = parts
            corrected_sentence = corrector.correct_sentence(incorrect_text.strip())
            predicted_tokens = corrected_sentence.split()
            output_file.write(f"GT  : {correct_text.strip()}\n")
            output_file.write(f"OUT : {' '.join(predicted_tokens)}\n")
            print("\n----------------------------------------")
            print("GT  :", correct_text.strip())
            print("OUT :", " ".join(predicted_tokens))